{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization techniques for recommender systems (As a Netflix Prize competition) \n",
    "### ğŸ”· Loss function \n",
    "- $\\min_{x^*, y^*}\\sum_{(u,i)\\in\\kappa}\\Big\\{\\big(r_{ui}-\\langle x_u,y_i\\rangle\\big)^2 + \\lambda\\big(\\|x_u\\|^2 + \\|y_i\\|^2\\big)\\Big\\}$  \n",
    "- $\\kappa = \\{(u,i)~|~1\\leq u\\leq m,~1\\leq i\\leq n\\}$ : the set of the $(u,i)$ pairs for which $r_{ui}$ is known (the training set)  \n",
    "\n",
    "### ğŸ”· Two approaches to minimizing above loss function\n",
    "1. Stochastic Gradient Descent (SGD) \n",
    "    - Given ratings for training set, $R = (r_{ui})\\in Mat_{m\\times n}$ \n",
    "    - Want : $\\widetilde{R}=XY^T\\approx R$, $\\widetilde{R}$ predicts $R$.\n",
    "    - Compute the associated prediction error :  \n",
    "        - $e_{ui}:= r_{ui} - \\langle x_u, y_i\\rangle$    \n",
    "2. Alternating Least Squares (ALS)  \n",
    "    - $x_u, y_i$ ëª¨ë‘ ë¯¸ì§€ìˆ˜ì´ë¯€ë¡œ ìœ„ì—ì„œ ì •ì˜í•œ loss functionì€ convexê°€ ì•„ë‹˜\n",
    "    - ë‘˜ ì¤‘ í•˜ë‚˜ë¥¼ ìƒìˆ˜ë¡œ ê³ ì •í•˜ë©´ ìµœì†Œì œê³± ë¬¸ì œ  \n",
    "    - ê° ë‹¨ê³„ê°€ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ìœ„ì˜ loss functionì€ ê°ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 4\n",
      "Number of items = 5\n",
      "User-latent (4,3)-Matirx X = \n",
      "[[ 0.54144845 -0.2039188  -0.17605725]\n",
      " [-0.35765621  0.28846921 -0.76717957]\n",
      " [ 0.58160392 -0.25373563  0.10634637]\n",
      " [-0.08312346  0.48736931 -0.68671357]]\n",
      "--------------------------------------------\n",
      "Item-latent (5,3)-Matirx Y = \n",
      "[[-0.1074724  -0.12801812  0.37792315]\n",
      " [-0.36663042 -0.05747607 -0.29261947]\n",
      " [ 0.01407125  0.19427174 -0.36687306]\n",
      " [ 0.38157457  0.30053024  0.16749811]\n",
      " [ 0.30028532 -0.22790929 -0.04096341]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# the number of factors (latent features)\n",
    "K = 3\n",
    "\n",
    "R = np.array([[4, np.NaN, np.NaN, 2, np.NaN],\n",
    "              [np.NaN, 5, np.NaN, 3, 1],\n",
    "              [np.NaN, np.NaN, 3, 4, 4],\n",
    "              [5, 2, 1, 2, np.NaN]]) \n",
    "num_users, num_items = R.shape\n",
    "print('Number of users = %d' % num_users, 'Number of items = %d' % num_items, sep='\\n')\n",
    "\n",
    "# X, Y random initialize\n",
    "np.random.seed(1)\n",
    "X = np.random.normal(scale=1.0/K, size=(num_users, K))\n",
    "Y = np.random.normal(scale=1.0/K, size=(num_items, K))\n",
    "print('User-latent (%d,%d)-Matirx X = ' % X.shape, X, sep='\\n')\n",
    "print('--------------------------------------------')\n",
    "print('Item-latent (%d,%d)-Matirx Y = ' % Y.shape, Y, sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### 1. SGDì„ ì´ìš©í•œ MF ì—…ë°ì´íŠ¸\n",
    "- ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ $\\gamma$ (learning rate)ì— ë¹„ë¡€í•˜ëŠ” í¬ê¸°ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ê²°ê³¼ ì¶œë ¥ :  \n",
    "    - $x_u ~\\leftarrow~ x_u + \\gamma(e_{ui}y_i - \\lambda x_u)$\n",
    "    - $y_i ~\\leftarrow~ y_i + \\gamma(e_{ui}x_u - \\lambda y_i)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step: 0, rmse: 3.238805\n",
      "iter step: 10, rmse: 2.916635\n",
      "iter step: 20, rmse: 2.198707\n",
      "iter step: 30, rmse: 1.304553\n",
      "iter step: 40, rmse: 0.756513\n",
      "iter step: 50, rmse: 0.487672\n",
      "iter step: 60, rmse: 0.356231\n",
      "iter step: 70, rmse: 0.279523\n",
      "iter step: 80, rmse: 0.226563\n",
      "iter step: 90, rmse: 0.187008\n",
      "iter step: 100, rmse: 0.156434\n",
      "iter step: 110, rmse: 0.132346\n",
      "iter step: 120, rmse: 0.113113\n",
      "iter step: 130, rmse: 0.097591\n",
      "iter step: 140, rmse: 0.084944\n",
      "iter step: 150, rmse: 0.074551\n",
      "iter step: 160, rmse: 0.065941\n",
      "iter step: 170, rmse: 0.058753\n",
      "iter step: 180, rmse: 0.052710\n",
      "iter step: 190, rmse: 0.047599\n"
     ]
    }
   ],
   "source": [
    "## SGD ë¥¼ ì´ìš©í•œ MF ì—…ë°ì´íŠ¸\n",
    "# ì‹¤ì œ R í–‰ë ¬ê³¼ ì˜ˆì¸¡ í–‰ë ¬ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def get_rmse(R, X, Y, non_zeros):\n",
    "    # error = 0\n",
    "    # latent rating matrix, predict matrix\n",
    "    latent_R = X @ Y.T\n",
    "\n",
    "    # ì—¬ê¸°ì„œ non_zerosëŠ” ì•„ë˜ í•¨ìˆ˜ì—ì„œ í™•ì¸\n",
    "    x_non_zero_ind = [non_zeros[0] for non_zeros in non_zeros]\n",
    "    y_non_zero_ind = [non_zeros[1] for non_zeros in non_zeros]\n",
    "\n",
    "    # ì› í–‰ë ¬ Rì—ì„œ 0 ì•„ë‹Œ ê°’ë“¤ë§Œ ì¶”ì¶œ\n",
    "    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n",
    "\n",
    "    # ì˜ˆì¸¡ í–‰ë ¬ì—ì„œ ì› í–‰ë ¬ Rì—ì„œ 0ì´ ì•„ë‹Œ ìœ„ì¹˜ì˜ ê°’ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥\n",
    "    latent_R_non_zeros = latent_R[x_non_zero_ind, y_non_zero_ind]\n",
    "\n",
    "    mse = mean_squared_error(R_non_zeros, latent_R_non_zeros)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def matrix_factorization(R, steps=200, learning_rate=0.01, r_lambda=0.01):\n",
    "    \n",
    "    # R>0ì¸ í–‰ ìœ„ì¹˜, ì—´ ìœ„ì¹˜, ê°’ì„ non_zeros ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "    non_zeros = [ (u, i, R[u, i]) for u in range(num_users)\n",
    "                  for i in range(num_items) if R[u, i] > 0 ]\n",
    "    \n",
    "    # SGD ê¸°ë²•ìœ¼ë¡œ X, Y ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸\n",
    "    for step in range(steps):\n",
    "        for u, i, r in non_zeros:\n",
    "            # ì”ì°¨ êµ¬í•¨\n",
    "            eui = r - X[u, :] @ Y[i, :].T\n",
    "\n",
    "            # Regularization(lambda-term)ì„ ë°˜ì˜í•œ SGD ì—…ë°ì´í„° ì ìš©\n",
    "            X[u, :] = X[u, :] + learning_rate*(eui * Y[i, :] - r_lambda*X[u, :])\n",
    "            Y[i, :] = Y[i, :] + learning_rate*(eui * X[u, :] - r_lambda*Y[i, :])\n",
    "\n",
    "        rmse = get_rmse(R, X, Y, non_zeros)\n",
    "        if step % 10 == 0:\n",
    "            print(\"iter step: {0}, rmse: {1:4f}\".format(step, rmse))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = matrix_factorization(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Latent Matrix, X = \n",
      " [[ 1.05573236  0.39757377 -0.77039218]\n",
      " [-0.06142198  0.94465111 -2.56395289]\n",
      " [ 2.4813266   0.13694622 -1.12174128]\n",
      " [ 0.5401693   1.11424815 -1.13896042]]\n",
      "-------------------------------------------------------------\n",
      "Item-Latent Matrix, Y = \n",
      " [[ 1.61888728  1.41169357 -2.19725822]\n",
      " [-0.89031381  0.42042905 -1.76736169]\n",
      " [ 1.01945627 -0.04623446 -0.42590928]\n",
      " [ 1.08128438  0.22631325 -1.09102838]\n",
      " [ 1.38928895 -0.23543601 -0.51337457]]\n"
     ]
    }
   ],
   "source": [
    "print('User-Latent Matrix, X = \\n', X)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Item-Latent Matrix, Y = \\n', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent rating matrix (Predicted rating matrix) =\n",
      "[[ 3.96311456  0.58878009  1.38600854  2.07204285  1.76861388]\n",
      " [ 6.86778919  4.98327571  0.98571887  2.94471781  1.00853046]\n",
      " [ 6.67506941 -0.16906059  3.00103235  3.93786401  3.99091101]\n",
      " [ 4.9500403   2.00049712  0.98425613  2.07888389  1.07283042]]\n",
      "---------------------------------------------------\n",
      "Given Rating Matrix R =\n",
      "[[ 4. nan nan  2. nan]\n",
      " [nan  5. nan  3.  1.]\n",
      " [nan nan  3.  4.  4.]\n",
      " [ 5.  2.  1.  2. nan]]\n"
     ]
    }
   ],
   "source": [
    "pred_matrix = np.dot(X, Y.T)\n",
    "print('Latent rating matrix (Predicted rating matrix) =', pred_matrix, sep='\\n')\n",
    "print('---------------------------------------------------')\n",
    "print('Given Rating Matrix R =', R, sep='\\n') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ALS ì•Œê³ ë¦¬ì¦˜ì— ì˜í•œ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Rating Matrix P =\n",
      "[[1 0 0 1 0]\n",
      " [0 1 0 1 1]\n",
      " [0 0 1 1 1]\n",
      " [1 1 1 1 0]]\n",
      "(User, Item) = (4, 5)\n",
      "-------------------------------------------------------------\n",
      "sparse matrix R : \n",
      " [[4. 0. 0. 2. 0.]\n",
      " [0. 5. 0. 3. 1.]\n",
      " [0. 0. 3. 4. 4.]\n",
      " [5. 2. 1. 2. 0.]]\n",
      "-------------------------------------------------------------\n",
      "confidence matrix C :\n",
      " [[161.   1.   1.  81.   1.]\n",
      " [  1. 201.   1. 121.  41.]\n",
      " [  1.   1. 121. 161. 161.]\n",
      " [201.  81.  41.  81.   1.]]\n"
     ]
    }
   ],
   "source": [
    "## equation 8 of paper - p. 47 \n",
    "\n",
    "#### Binary rating matrix = P : bipartite adjacency block\n",
    "P = np.copy(R)\n",
    "P = np.where(P > 0, 1, 0)\n",
    "print('Binary Rating Matrix P =', P, '(User, Item) = (%d, %d)' % (P.shape[0], P.shape[1]), sep='\\n')\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "#### sparse rating matrix : ê²°ì¸¡ê°’ì€ 0ìœ¼ë¡œ ì±„ì›Œì„œ sparse matrixë¡œ ë§Œë“¦ \n",
    "sparse_R = np.where(R > 0, R, 0)\n",
    "print('sparse matrix R : \\n', sparse_R) \n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "#### increasing rate of confidence in p_ui=1 (i.e., positive preference)\n",
    "alpha = 40\n",
    "#### confidence matrix \n",
    "C = 1 + (alpha * sparse_R) \n",
    "print('confidence matrix C :\\n', C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://yeomko.tistory.com/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cu = np.diag(C[0])\n",
    "# print(Cu.shape, Y.shape, (Y.T @ Y).shape)\n",
    "# # for i in range(num_items):\n",
    "# pu = np.array([X[0] @ Y[i].T for i in range(num_items)])\n",
    "# pi = np.array([Y[0] @ X[u].T for u in range(num_users)])\n",
    "# print(pu.shape, pi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------step 0----------------\n",
      "predict error: 99.292816\n",
      "regularization: 1560.720443\n",
      "total loss: 1660.013259\n",
      "------------step 1----------------\n",
      "predict error: 128.769788\n",
      "regularization: 16.044941\n",
      "total loss: 144.814729\n",
      "------------step 2----------------\n",
      "predict error: 129.997547\n",
      "regularization: 0.028166\n",
      "total loss: 130.025713\n",
      "------------step 3----------------\n",
      "predict error: 129.999995\n",
      "regularization: 0.000059\n",
      "total loss: 130.000054\n",
      "------------step 4----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 5----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 6----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 7----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 8----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 9----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 10----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 11----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 12----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 13----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n",
      "------------step 14----------------\n",
      "predict error: 130.000000\n",
      "regularization: 0.000000\n",
      "total loss: 130.000000\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ loss function \n",
    "def basic_loss_function(sparse_R, X, Y, r_lambda=40):\n",
    "    # predict error = (rui - xu.T @ yi)^2 \n",
    "    pred_error = np.sum(np.square(sparse_R - X @ Y.T))\n",
    "    # regularization term = lambda * (sum xu-norm + sum yi-norm) : ì •ê·œí™”\n",
    "    regular = r_lambda * (np.trace(X @ X.T) + np.trace(Y @ Y.T))\n",
    "    # total loss = pred_error + reg \n",
    "    total_loss = pred_error + regular\n",
    "    return pred_error, regular, total_loss\n",
    "\n",
    "# optimizer (|users| = 4, |items|=5)\n",
    "# xu = (Y.T_Y + lambda * I)^{-1} Y.T_ru\n",
    "# ru = [ru1, ru2, ... ,ru5].T : (5,1)-matrix (or 5-dim vector)\n",
    "def basic_opt_user(X, Y, sparse_R, num_users, K=3, r_lambda=40):\n",
    "    for u in range(num_users):\n",
    "        YT_Y = Y.T @ Y\n",
    "        lI = 40 * np.identity(K)\n",
    "        YT_ru = Y.T @ sparse_R[u]\n",
    "        X[u] = np.linalg.solve(YT_Y + lI, YT_ru)\n",
    "\n",
    "def basic_opt_item(X, Y, sprase_R, num_items, K=3, r_lambda=40):\n",
    "    for i in range(num_items):\n",
    "        XT_X = X.T @ X\n",
    "        lI = 40 * np.identity(K)\n",
    "        XT_ri = X.T @ sparse_R[:, i]\n",
    "        Y[i] = np.linalg.solve(XT_X + lI, XT_ri)\n",
    "\n",
    "# train \n",
    "# usually ALS algorithm repeat train steps for 10~15 times\n",
    "b_pred_errors, b_regular_list, b_total_losses = [], [], []\n",
    "\n",
    "for i in range(15):\n",
    "    if i != 0:\n",
    "        basic_opt_user(X, Y, sparse_R, num_users)\n",
    "        basic_opt_item(X, Y, sparse_R, num_items)\n",
    "    pred = X @ Y.T\n",
    "    pred_error, regular, total_loss = basic_loss_function(sparse_R, X, Y)\n",
    "\n",
    "    b_pred_errors.append(pred_error)\n",
    "    b_regular_list.append(regular)\n",
    "    b_total_losses.append(total_loss)\n",
    "    print('------------step %d----------------' % i)\n",
    "    print('predict error: %f' % pred_error)\n",
    "    print('regularization: %f' % regular)\n",
    "    print('total loss: %f' % total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction for basic als =\n",
      " [[2.81561090e-37 2.34468302e-37 1.37004412e-37 3.53142074e-37\n",
      "  1.66142209e-37]\n",
      " [3.68427923e-37 3.06806135e-37 1.79272821e-37 4.62092973e-37\n",
      "  2.17400171e-37]\n",
      " [3.82292064e-37 3.18351414e-37 1.86018954e-37 4.79481780e-37\n",
      "  2.25581056e-37]\n",
      " [4.17922123e-37 3.48022131e-37 2.03356134e-37 5.24170031e-37\n",
      "  2.46605469e-37]]\n",
      "---------------------------------------------------\n",
      "Given Rating Matrix R =\n",
      "[[ 4. nan nan  2. nan]\n",
      " [nan  5. nan  3.  1.]\n",
      " [nan nan  3.  4.  4.]\n",
      " [ 5.  2.  1.  2. nan]]\n"
     ]
    }
   ],
   "source": [
    "basic_predict = X @ Y.T\n",
    "print('Final prediction for basic als =\\n', basic_predict)\n",
    "print('---------------------------------------------------')\n",
    "print('Given Rating Matrix R =', R, sep='\\n') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "### 3. weighted ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------step 0----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 1----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 2----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 3----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 4----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 5----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 6----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 7----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1452.000000\n",
      "regularization: 0.000000\n",
      "total loss: 1452.000000\n",
      "------------step 8----------------\n",
      "predict error: 12.000000\n",
      "confidence error: 1451.999972\n",
      "regularization: 0.000014\n",
      "total loss: 1451.999986\n",
      "------------step 9----------------\n",
      "predict error: 11.998724\n",
      "confidence error: 1451.840960\n",
      "regularization: 0.080570\n",
      "total loss: 1451.921531\n",
      "------------step 10----------------\n",
      "predict error: 7.515396\n",
      "confidence error: 861.510945\n",
      "regularization: 259.460576\n",
      "total loss: 1120.971521\n",
      "------------step 11----------------\n",
      "predict error: 5.115373\n",
      "confidence error: 66.712607\n",
      "regularization: 307.587781\n",
      "total loss: 374.300387\n",
      "------------step 12----------------\n",
      "predict error: 5.432809\n",
      "confidence error: 48.028002\n",
      "regularization: 300.423059\n",
      "total loss: 348.451061\n",
      "------------step 13----------------\n",
      "predict error: 5.583214\n",
      "confidence error: 40.465217\n",
      "regularization: 299.781019\n",
      "total loss: 340.246236\n",
      "------------step 14----------------\n",
      "predict error: 5.666057\n",
      "confidence error: 36.813113\n",
      "regularization: 300.696990\n",
      "total loss: 337.510103\n"
     ]
    }
   ],
   "source": [
    "# weighted loss function\n",
    "def loss_function(C, P, X, Y, r_lambda=40):\n",
    "    # predict error = (pui - xu.T @ yi)^2 : 0ê³¼ 1ë¡œ ë‚˜ëˆ„ì–´ ì„ í˜¸/ë¹„ì„ í˜¸ë¥¼ ì˜ˆì¸¡í•œ ê²°ê³¼ì˜ ì˜¤ì°¨\n",
    "    pred_error = np.square(P - X @ Y.T)\n",
    "    # confidence error = cui (pui - xu.T @ yi)^2 : ì‹ ë¢°ìˆ˜ì¤€ì„ ì ìš©í•œ ì˜ˆì¸¡ê°’ì— ëŒ€í•œ ì˜¤ì°¨\n",
    "    conf_error = np.sum(C * pred_error)\n",
    "    # regularization term = lambda * (sum xu-norm + sum yi-norm) : ì •ê·œí™”\n",
    "    regular = r_lambda * (np.trace(X @ X.T) + np.trace(Y @ Y.T))\n",
    "    # total loss = conf_error + reg = (conf_level * pred_error) + reg\n",
    "    total_loss = conf_error + regular\n",
    "    return np.sum(pred_error), conf_error, regular, total_loss\n",
    "\n",
    "# optimizer (|users| = 4, |items|=5)\n",
    "# xu = (Y.T_Cu_Y + lambda * I)^{-1} Y.T_Cu_pu\n",
    "# pu = [pu1, pu2, ... , pu5].T : (5,1)-matrix (or 5-dim vector)\n",
    "def opt_user(X, Y, C, P, num_users, K=3, r_lambda=40):\n",
    "    for u in range(num_users):\n",
    "        # Y : (5,3)-matrix (items, factors)\n",
    "        Cu = np.diag(C[u]) # (5,5)-diagonal matrix\n",
    "        YT_Cu_Y = Y.T @ Cu @ Y # (3,3)-mat\n",
    "        lI = r_lambda * np.identity(K) \n",
    "        # pu = np.array([X[u] @ Y[i].T for i in range(num_items)])\n",
    "        YT_Cu_pu = Y.T @ Cu @ P[u] #pu\n",
    "        X[u] = np.linalg.solve(YT_Cu_Y + lI, YT_Cu_pu)\n",
    "\n",
    "def opt_item(X, Y, C, P, num_items, K=3, r_lambda=40):\n",
    "    for i in range(num_items):\n",
    "        # X : (4,3)-matrix (users, factors)\n",
    "        Ci = np.diag(C[:, i]) # (4,4)-diagonal matrix\n",
    "        XT_Ci_X = X.T @ Ci @ X # (3,3)-mat\n",
    "        lI = r_lambda * np.identity(K)\n",
    "        # pi = np.array([Y[0] @ X[u].T for u in range(num_users)])\n",
    "        XT_Ci_pi = X.T @ Ci @ P[:, i] #pi\n",
    "        Y[i] = np.linalg.solve(XT_Ci_X + lI, XT_Ci_pi)\n",
    "\n",
    "# train \n",
    "# usually ALS algorithm repeat train steps for 10~15 times\n",
    "pred_errors, conf_errors, regular_list, total_losses = [], [], [], []\n",
    "\n",
    "for i in range(15):\n",
    "    if i != 0:\n",
    "        opt_user(X, Y, C, P, num_users)\n",
    "        opt_item(X, Y, C, P, num_items)\n",
    "    pred = X @ Y.T\n",
    "    pred_error, conf_error, regular, total_loss = loss_function(C, P, X, Y)\n",
    "\n",
    "    pred_errors.append(pred_error)\n",
    "    conf_errors.append(conf_error)\n",
    "    regular_list.append(regular)\n",
    "    total_losses.append(total_loss)\n",
    "    print('------------step %d----------------' % i)\n",
    "    print('predict error: %f' % pred_error)\n",
    "    print('confidence error: %f' % conf_error)\n",
    "    print('regularization: %f' % regular)\n",
    "    print('total loss: %f' % total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction =\n",
      " [[0.83035728 0.78056955 0.68670556 0.82108667 0.71520758]\n",
      " [0.90964559 0.85510378 0.75227699 0.89948975 0.78350059]\n",
      " [0.96595371 0.9080357  0.79884382 0.95516922 0.8320002 ]\n",
      " [0.90500029 0.85073701 0.74843533 0.89489632 0.77949948]]\n",
      "---------------------------------------------------\n",
      "Given Rating Matrix R =\n",
      "[[ 4. nan nan  2. nan]\n",
      " [nan  5. nan  3.  1.]\n",
      " [nan nan  3.  4.  4.]\n",
      " [ 5.  2.  1.  2. nan]]\n"
     ]
    }
   ],
   "source": [
    "predict = X @ Y.T\n",
    "print('Final prediction =\\n', predict)\n",
    "print('---------------------------------------------------')\n",
    "print('Given Rating Matrix R =', R, sep='\\n') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------\n",
    "### 4. ALS ì°¸ê³  : `csr_matrix` ì´ìš©í•œ ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ALS(sparse_R, X, Y, r_lambda = 40, alpha = 40, n_iter = 10, rank_size = K):\n",
    "    import scipy\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    # Confidence matrix : C = 1+ alpha * r_{ui}\n",
    "    conf = 1 + (alpha * sparse_R) # sparse í–‰ë ¬ í˜•íƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ 1ì„ ë‚˜ì¤‘ì— ë”í•¨\n",
    "\n",
    "    # X, Y ì´ˆê¸°í™”\n",
    "    X = csr_matrix(X)\n",
    "    Y = csr_matrix(Y)\n",
    "    X_I = scipy.sparse.eye(num_users)\n",
    "    Y_I = scipy.sparse.eye(num_items)\n",
    "    \n",
    "    # ì •ê·œí™” term: ğ€I -> (3,3)-identity matrix * lambda\n",
    "    lambda_I = r_lambda * scipy.sparse.eye(rank_size)\n",
    "    \n",
    "    # ë°˜ë³µ ì‹œì‘\n",
    "    for i in range(n_iter):\n",
    "        xTx = X.T @ X\n",
    "        yTy = Y.T @ Y\n",
    "        \n",
    "        # Y(item)ë¥¼ ê³ ì •í•´ë†“ê³  X(user)ì— ëŒ€í•´ ë°˜ë³µ\n",
    "        # Xu = (yTy + yT(Cu-I)Y + ğ€I)^{-1} yTCuPu\n",
    "        for u in range(num_users):\n",
    "            conf_samp = conf[u,:] # Cu\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref!=0] = 1\n",
    "            # Cu-I: ìœ„ì—ì„œ confì— 1ì„ ë”í•˜ì§€ ì•Šì•˜ìœ¼ë‹ˆê¹Œ Ië¥¼ ë¹¼ì§€ ì•ŠìŒ \n",
    "            CuI = scipy.sparse.diags(conf_samp, 0)\n",
    "            # yT(Cu-I)Y\n",
    "            yTCuIY = Y.T @ CuI @ Y\n",
    "            # yTCuPu\n",
    "            yTCupu = Y.T @ CuI @ pref.T\n",
    "            # solve the equation \n",
    "            X[u] = scipy.sparse.linalg.spsolve(yTy + yTCuIY + lambda_I, yTCupu)\n",
    "        \n",
    "        # Xë¥¼ ê³ ì •í•´ë†“ê³  Yì— ëŒ€í•´ ë°˜ë³µ\n",
    "        # Yi = (xTx + xT(Cu-I)X + ğ€I)^{-1} xTCiPi\n",
    "        for i in range(num_items):\n",
    "            conf_samp = conf[:,i].T#.toarray()\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref!=0] = 1\n",
    "            \n",
    "            #Ci-I\n",
    "            CiI = scipy.sparse.diags (conf_samp, 0)\n",
    "            # xT(Ci-I)X\n",
    "            xTCiIX = X.T @ CiI @ X\n",
    "            # xTCiPi\n",
    "            xTCiPi = X.T @ CiI @ pref.T\n",
    "            \n",
    "            Y[i] = scipy.sparse.linalg.spsolve(xTx + xTCiIX + lambda_I, xTCiPi)\n",
    "            \n",
    "        return X, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction of ALS-model =\n",
      " [[0.83090211 0.78122541 0.68792949 0.81800898 0.71609303]\n",
      " [0.90930751 0.85494323 0.75284374 0.89519776 0.78366485]\n",
      " [0.96535566 0.90764045 0.79924773 0.95037621 0.83196859]\n",
      " [0.90408568 0.8500336  0.74852043 0.89005696 0.77916454]]\n",
      "---------------------------------------------------\n",
      "Given Rating Matrix R =\n",
      "[[ 4. nan nan  2. nan]\n",
      " [nan  5. nan  3.  1.]\n",
      " [nan nan  3.  4.  4.]\n",
      " [ 5.  2.  1.  2. nan]]\n"
     ]
    }
   ],
   "source": [
    "X, Y_T = weighted_ALS(sparse_R, X, Y)\n",
    "pred_als = X @ Y_T\n",
    "print('Final prediction of ALS-model =\\n', pred_als.toarray())\n",
    "print('---------------------------------------------------')\n",
    "print('Given Rating Matrix R =', R, sep='\\n') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "### 5. `implicit` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš©í•œ ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012461185455322266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 15,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a46c1623f92436a880f29eb44bfc46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using `implicit` library for ALS\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares as ALS\n",
    "\n",
    "alpha = 40\n",
    "# user_vectors = xu, item_vectors = yi\n",
    "inp = csr_matrix(alpha * sparse_R)\n",
    "als_model = ALS(factors=3, regularization=40, iterations=15)\n",
    "pred = als_model.fit(inp)\n",
    "# pred ì´ìš©í•´ì„œ rmse ê³„ì‚° ë“±ë“±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
